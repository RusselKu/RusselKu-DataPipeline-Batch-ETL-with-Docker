````markdown
# ğŸš€ ETL Pipeline: Airflow + MongoDB + Streamlit

A fully dockerized batch ETL pipeline that:
- â¬‡ï¸ **Ingests** data from multiple public APIs via Apache Airflow
- ğŸ—ƒï¸ **Stores** both raw and processed data in MongoDB
- ğŸ“Š **Visualizes** results using interactive Streamlit dashboards

## ğŸ“Œ Project Overview

This project demonstrates a modular, scalable data pipeline architecture using Airflow for orchestration, MongoDB for data persistence, and Streamlit for visualization. It is ideal for educational and production-ready use cases where asynchronous data ingestion and fast dashboarding are required.

---

## ğŸŒ APIs Used

- ğŸ’± **Exchange Rate API (Fawaz Ahmed)**  

  Provides exchange rates for hundreds of currencies in JSON format.

- ğŸª™ **Gold Price API (Metals API clone)**  
  â†ª Simulated API for retrieving historical gold prices.

---

## ğŸ†š Why MongoDB over PostgreSQL?

MongoDB was chosen for its schema flexibility and native support for nested JSON documents â€” making it ideal for storing hierarchical data from external APIs without the need for schema migrations. It also allows rapid prototyping and dynamic visualizations in Streamlit.

---

## ğŸ³ How to Launch Services

From the project root:

```bash
docker-compose up --build
````

This will launch the following services:

| Service       | URL                                            |
| ------------- | ---------------------------------------------- |
| Airflow UI    | [http://localhost:8080](http://localhost:8080) |
| Streamlit App | [http://localhost:8501](http://localhost:8501) |
| MongoDB       | localhost:27017 (internal)                     |

---

## ğŸ“… How to Trigger DAGs

1. Open the Airflow UI at:
   â†’ [http://localhost:8080](http://localhost:8080)

2. Use the toggle switch to **enable DAGs** like:

   * `CurrenlyExchangeRates_ingestion`
   * `Gold_Exchangeingestion`
   * `main_pipeline`

3. Click the â–¶ï¸ **Trigger DAG** button.

4. Navigate to the "Graph View" or "Tree View" to check execution flow.

---

## ğŸªµ How to View Airflow Logs

* In the Airflow UI, select a DAG run.
* Click on any task (e.g., `extract_exchange_rates`).
* Then click **"View Log"** to inspect output, errors, or XComs.

---

## ğŸ“ˆ Open the Streamlit Dashboard

Once services are running:

â†’ Visit [http://localhost:8501](http://localhost:8501)

Youâ€™ll see multiple dashboards for:

* ğŸ“‰ **Exchange Rate Trends**
* ğŸª™ **Gold Price Visualizations**
* ğŸŒ **Currency Metadata**

Each dashboard allows interactive filtering and plot generation in real-time.

---

## ğŸ”„ How XCom is Used

Apache Airflowâ€™s **XCom** (cross-communication) mechanism is used to:

* Pass processed exchange rate results between tasks.
* Share MongoDB insertion results across tasks for logging and conditional logic.
* Track DAG execution metadata that can be surfaced in dashboards.

---

## ğŸ“‚ Folder Structure

```bash
pipeline_chaos_to_insights/
â”œâ”€â”€ dags/                       # All Airflow DAG definitions
â”œâ”€â”€ utils/                      # Shared ETL helpers (Mongo, APIs, transforms)
â”œâ”€â”€ streamlit_app/             # Dashboards, pages, styling
â”‚   â”œâ”€â”€ pages/
â”‚   â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ styles/
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ Documentation-Initialization/
â”‚   â””â”€â”€ README.md               # DAG-level doc with images (if needed)
â”œâ”€â”€ docker-compose.yml         # Defines all services
â”œâ”€â”€ Dockerfile                 # Airflow base Docker image
â””â”€â”€ .env                       # Your local secrets (excluded by .gitignore)
```

---

## âœ… To-Do / Coming Soon

* [ ] Add unit tests for pipeline components
* [ ] Include screenshots of dashboards
* [ ] Add support for PostgreSQL as optional DB backend
* [ ] CI/CD with GitHub Actions

---

Built with â¤ï¸ by [Russel Ku](https://github.com/RusselKu)

