# Docker Compose file for Apache Airflow
# This file tells Docker how to run multiple applications together
version: '3.8'

services:
  # PostgreSQL Database
  # This is where Airflow stores all its information (DAGs, task history, etc.)
  postgres:
    image: postgres:13                    # Use PostgreSQL version 13
    environment:                         # Set up database credentials
      POSTGRES_USER: airflow             # Database username
      POSTGRES_PASSWORD: airflow         # Database password  
      POSTGRES_DB: airflow               # Database name
    volumes:                             # Save data permanently
      - postgres_data:/var/lib/postgresql/data  # Store database files


# MongoDB Service
  mongodb:
    image: mongo:6.0
    ports:
      - "27017:27017"
    volumes:
      - mongodb_data:/data/db
    environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: example


  # Airflow Webserver
  # This is the web interface you see in your browser
  webserver:
    build: .          # Use Airflow version 2.8.1
    depends_on:                          # Wait for postgres to start first
      - postgres
    environment:                         # Configure Airflow settings
      AIRFLOW__CORE__EXECUTOR: LocalExecutor    # Use local executor (simpler)
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow  # Connect to postgres
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'     # Don't load example DAGs
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'  # Pause new DAGs by default
      PYTHONPATH: /opt/airflow
      # Conexión a MongoDB (añadido)
      AIRFLOW__MONGO__CONN_URI: "mongodb://root:example@mongodb:27017/project_db"
      AIRFLOW__CORE__PLUGINS_FOLDER: "/opt/airflow/utils"  # <-- LÍNEA NUEVA 1
    volumes:                             # Connect local folders to container
      - ./dags:/opt/airflow/dags
      - ./utils:/opt/airflow/utils         # Your DAG files go here
      - ./logs:/opt/airflow/logs
      - ./requirements.txt:/requirements.txt        # Airflow logs go here
    ports:                               # Make webserver accessible from browser
      - "8080:8080"                      # Port 8080 on your computer -> port 8080 in container
    command: webserver                   # Start the webserver

  # Airflow Scheduler  
  # This is the "brain" that runs your tasks
  scheduler:
    build: .          # Same Airflow image
    depends_on:                          # Wait for postgres to start first
      - postgres
    environment:                         # Same configuration as webserver
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      PYTHONPATH: /opt/airflow
      # Conexión a MongoDB (añadido)
      AIRFLOW__MONGO__CONN_URI: "mongodb://root:example@mongodb:27017/project_db"
      AIRFLOW__CORE__PLUGINS_FOLDER: "/opt/airflow/utils"  # <-- LÍNEA NUEVA 2
    volumes:                             # Same folder connections
      - ./dags:/opt/airflow/dags
      - ./utils:/opt/airflow/utils 
      - ./logs:/opt/airflow/logs
    command: scheduler                   # Start the scheduler

# Streamlit Dashboard
  streamlit:
    build:
      context: ./streamlit_app
      dockerfile: Dockerfile
    depends_on:
      - mongodb
    volumes:
      - ./streamlit_app:/app
    ports:
      - "8501:8501"
    environment:
      # DB especificada (modificado)
      - MONGO_URI=mongodb://root:example@mongodb:27017/project_db


# Volumes - This is where Docker stores data permanently
volumes:
  postgres_data:
  mongodb_data: